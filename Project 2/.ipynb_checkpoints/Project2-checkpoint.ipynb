{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e653a7e7-035c-4ce9-af30-5bf1b62cb624",
   "metadata": {},
   "source": [
    "## PROJECT 2 - MACHINE LEARNING \n",
    "# DATA ANALYTICS, VISUALIZATION AND CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af258620-25f5-49a5-8e36-1e06ddf53a92",
   "metadata": {},
   "source": [
    "In this project, you will be analyzing a dataset of customer data for a marketing campaign. The dataset contains data on customer demographics, purchase behaviors, and marketing campaign responses. One common application for this type of data is Customer Segmentation: using demographics and purchasing behaviors to cluster customers into distinct groups for targeted marketing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f60ef-cc6f-4649-ae6e-d05886416142",
   "metadata": {},
   "source": [
    "### PART 1 - EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026d66b-0b0a-4bce-a10d-8bcacbd6f3ba",
   "metadata": {},
   "source": [
    "Import the dataset with pandas. Then, perform the following steps for exploratory data analysis:\n",
    "- How many columns are there in the dataset? List the name of the columns.\n",
    "- What type of data is in each column?\n",
    "- Display the first 5 rows from the top and the first 5 rows from the bottom of the dataset.\n",
    "- What is the shape of the dataset?\n",
    "- Provide a statistical summary of dataset (mean, standard deviation, max, min, quartiles).\n",
    "- What is the number of missing values for each column?\n",
    "- Are there any duplicate rows?\n",
    "- Display a synthetic summary of the information pertaining the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3a5d6f-7cd6-404d-89da-88891f64d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('marketing_campaign.csv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83128531-5dfb-4503-a941-9498fda63bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>Dt_Customer</th>\n",
       "      <th>Recency</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>...</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "      <th>AcceptedCmp3</th>\n",
       "      <th>AcceptedCmp4</th>\n",
       "      <th>AcceptedCmp5</th>\n",
       "      <th>AcceptedCmp1</th>\n",
       "      <th>AcceptedCmp2</th>\n",
       "      <th>Complain</th>\n",
       "      <th>Z_CostContact</th>\n",
       "      <th>Z_Revenue</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5524</td>\n",
       "      <td>1957</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>04-09-2012</td>\n",
       "      <td>58</td>\n",
       "      <td>635</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2174</td>\n",
       "      <td>1954</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>08-03-2014</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4141</td>\n",
       "      <td>1965</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Together</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21-08-2013</td>\n",
       "      <td>26</td>\n",
       "      <td>426</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6182</td>\n",
       "      <td>1984</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Together</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10-02-2014</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5324</td>\n",
       "      <td>1981</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19-01-2014</td>\n",
       "      <td>94</td>\n",
       "      <td>173</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>10870</td>\n",
       "      <td>1967</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Married</td>\n",
       "      <td>61223.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13-06-2013</td>\n",
       "      <td>46</td>\n",
       "      <td>709</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>4001</td>\n",
       "      <td>1946</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Together</td>\n",
       "      <td>64014.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10-06-2014</td>\n",
       "      <td>56</td>\n",
       "      <td>406</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>7270</td>\n",
       "      <td>1981</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>56981.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25-01-2014</td>\n",
       "      <td>91</td>\n",
       "      <td>908</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>8235</td>\n",
       "      <td>1956</td>\n",
       "      <td>Master</td>\n",
       "      <td>Together</td>\n",
       "      <td>69245.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24-01-2014</td>\n",
       "      <td>8</td>\n",
       "      <td>428</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>9405</td>\n",
       "      <td>1954</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>52869.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15-10-2012</td>\n",
       "      <td>40</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2240 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Year_Birth   Education Marital_Status   Income  Kidhome  \\\n",
       "0      5524        1957  Graduation         Single  58138.0        0   \n",
       "1      2174        1954  Graduation         Single  46344.0        1   \n",
       "2      4141        1965  Graduation       Together  71613.0        0   \n",
       "3      6182        1984  Graduation       Together  26646.0        1   \n",
       "4      5324        1981         PhD        Married  58293.0        1   \n",
       "...     ...         ...         ...            ...      ...      ...   \n",
       "2235  10870        1967  Graduation        Married  61223.0        0   \n",
       "2236   4001        1946         PhD       Together  64014.0        2   \n",
       "2237   7270        1981  Graduation       Divorced  56981.0        0   \n",
       "2238   8235        1956      Master       Together  69245.0        0   \n",
       "2239   9405        1954         PhD        Married  52869.0        1   \n",
       "\n",
       "      Teenhome Dt_Customer  Recency  MntWines  ...  NumWebVisitsMonth  \\\n",
       "0            0  04-09-2012       58       635  ...                  7   \n",
       "1            1  08-03-2014       38        11  ...                  5   \n",
       "2            0  21-08-2013       26       426  ...                  4   \n",
       "3            0  10-02-2014       26        11  ...                  6   \n",
       "4            0  19-01-2014       94       173  ...                  5   \n",
       "...        ...         ...      ...       ...  ...                ...   \n",
       "2235         1  13-06-2013       46       709  ...                  5   \n",
       "2236         1  10-06-2014       56       406  ...                  7   \n",
       "2237         0  25-01-2014       91       908  ...                  6   \n",
       "2238         1  24-01-2014        8       428  ...                  3   \n",
       "2239         1  15-10-2012       40        84  ...                  7   \n",
       "\n",
       "      AcceptedCmp3  AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  \\\n",
       "0                0             0             0             0             0   \n",
       "1                0             0             0             0             0   \n",
       "2                0             0             0             0             0   \n",
       "3                0             0             0             0             0   \n",
       "4                0             0             0             0             0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "2235             0             0             0             0             0   \n",
       "2236             0             0             0             1             0   \n",
       "2237             0             1             0             0             0   \n",
       "2238             0             0             0             0             0   \n",
       "2239             0             0             0             0             0   \n",
       "\n",
       "      Complain  Z_CostContact  Z_Revenue  Response  \n",
       "0            0              3         11         1  \n",
       "1            0              3         11         0  \n",
       "2            0              3         11         0  \n",
       "3            0              3         11         0  \n",
       "4            0              3         11         0  \n",
       "...        ...            ...        ...       ...  \n",
       "2235         0              3         11         0  \n",
       "2236         0              3         11         0  \n",
       "2237         0              3         11         0  \n",
       "2238         0              3         11         0  \n",
       "2239         0              3         11         1  \n",
       "\n",
       "[2240 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cf467-b5b4-4c1a-a35c-9d94d865631c",
   "metadata": {},
   "source": [
    "### PART 2 - DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c4e35-629f-4af1-848a-fe0bb15f254b",
   "metadata": {},
   "source": [
    "Now that you have a rough idea of what your dataset contains, let's perform some basic data cleaning strategies!\n",
    "- Handle not a number (Nan) values. This can be done in two ways: either by removing all rows with Nan values, or by imputing the missing values. Show 1) how to remove the rows with Nan and 2) impute the missing values by substituting for each entry the mean value of that column by 'Education' class (you will have a different number based on how educated the person is). Hint: try the 'groupby' method!\n",
    "- Remove duplicate rows (if any).\n",
    "- Change the column \"Year_Birth\" to a new column \"Age\", with the appropriate transformation.\n",
    "- Combine the columns \"MntWines\", \"MntFruits\", \"MntMeatProducts\", \"MntFishProducts\", \"MntSweetProducts\", \"MntGoldProds\" into a column \"Total expenses\", with the appropriate transformation.\n",
    "- Drop the columns 'Education','Marital_Status' from the current version of the Dataframe, and then 'Dt_Customer' count outliers for each remaining column (the function is provided for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b54aa-2ce6-4a77-8d47-4e0edcbd9baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_outliers(data,col):\n",
    "        a = []\n",
    "        q1 = data[col].quantile(0.25,interpolation='nearest')\n",
    "        q2 = data[col].quantile(0.5,interpolation='nearest')\n",
    "        q3 = data[col].quantile(0.75,interpolation='nearest')\n",
    "        q4 = data[col].quantile(1,interpolation='nearest')\n",
    "        IQR = q3 -q1\n",
    "        global LLP\n",
    "        global ULP\n",
    "        LLP = q1 - 1.5*IQR\n",
    "        ULP = q3 + 1.5*IQR\n",
    "        if data[col].min() > LLP and data[col].max() < ULP:\n",
    "            print(\"No outliers in\",i)\n",
    "        else:\n",
    "            print(\"There are outliers in\",i)\n",
    "            x = data[data[col]<LLP][col].size\n",
    "            y = data[data[col]>ULP][col].size\n",
    "            a.append(i)\n",
    "            print('Count of outliers are:',x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0150f6a-7ad6-4e16-9571-520a8d917383",
   "metadata": {},
   "source": [
    "### PART 3 - CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f114858-3844-4061-9f48-9c0cb1faa0c0",
   "metadata": {},
   "source": [
    "Now that you have pre-processed your data, it is time for the actual clustering!\n",
    "First, we will perform PCA for dimensionality reduction. PCA is a technique for increasing interpretability but at the same time minimizing information loss. This procedure will make our clustering analysis more effective and interpretable. In order to perform PCA, we need to first encode the categorical labels ('Education','Marital_Status', 'Dt_Customer') into numerical ones. You can do so with 'LabelEncoder': https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93718e4f-b41f-41bc-9ffd-f6b60b1e9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "obj = ['Education','Marital_Status', 'Dt_Customer']\n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e26bb7-6bf1-406c-8524-5e63e1302980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "del_cols = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', \n",
    "            'Complain', 'Response']\n",
    "ds = df.drop(del_cols, axis=1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(ds)\n",
    "scaled_features = pd.DataFrame(scaler.transform(ds),columns= ds.columns )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891c41c-cf35-4af1-9f49-5a9640ffbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(scaled_features)\n",
    "PCA_df = pd.DataFrame(pca.transform(scaled_features), columns=([\"Education\",\"Income\", \"Kidhome\"]))\n",
    "PCA_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327da50-e794-47c0-9e00-8b0388d03fd8",
   "metadata": {},
   "source": [
    "We will use the PCA_df dataset for clustering analysis. Now that we have prepared our data, we can go ahead with the following:\n",
    "\n",
    "- Find optimal number of clusters with Elbow method, based on the K-means algorithm.\n",
    "- Perform k-means clustering with the optimal number of clusters and visualize the results. What are the cluster centroids?\n",
    "- How many elements are in each of the clusters?\n",
    "- Perform Gaussian Mixture model clustering and visualize the results. What are the cluster centroids now? What do they correspond to, in the GMM model? Is the number of clusters identified with K-means still adequate for the GMM model? Try rerunning the training of the model: do you get a different result?\n",
    "- Try out one of the following methods: DBSCAN, Mini-batch K-means, Mean Shift. Perform a little research on its functioning and summarize it in a Markdown cell.\n",
    "- Answer the following: which method performed best, and why do you think that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb75eb0-9a3d-4643-8c14-c0b1aea41f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d14eeb-f90c-4f1e-8f74-b31becaeedce",
   "metadata": {},
   "source": [
    "### PART 4 - ANOMALY DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ec5bf-b14b-4333-b3f8-1defee0a7c43",
   "metadata": {},
   "source": [
    "Good job for getting to the last part of the project! Now, we want to try to detect anomalies, that is, the outliers, with respect to the probability distributions we have identified with the GMM. Basically we want to find those customers who are unlikely to be part of the identified probability distribution, and might need further analysis (who knows, maybe they're psychopaths and you want to refer them to the closest police station). \n",
    "Do the following:\n",
    "- Extract the log probabilities from the GMM model you have trained.\n",
    "- Get the minimum and maximum probability.\n",
    "- Identify the outliers (use -10 as a threshold).\n",
    "- In your cluster label column, in the dataframe, add another label for those datapoints that are identified as outliers. Then, plot all your datapoints, and use this new column as color code. In this way you will see which element corresponds to each cluster and whether they are outliers or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa814d5e-f5d6-4d64-aaf3-a97d62b8a13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0de3eb10-34f5-42fc-99ed-6fd97bf9ae33",
   "metadata": {},
   "source": [
    "Congrats for getting to the end of your assignment! Now I unleash thee into the world, go teach the people the wisdom of data science! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ef391-717b-4915-a199-217c3151dfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
